#!/usr/bin/env node

/**
 * å¢å¼ºç‰ˆæ•°æ®å¯¼å‡º/å¯¼å…¥å·¥å…·
 * æ”¯æŒåŠ å¯†æ•°æ®çš„å¤„ç†
 */

const fs = require('fs').promises
const crypto = require('crypto')
const redis = require('../src/models/redis')
const logger = require('../src/utils/logger')
const readline = require('readline')
const config = require('../config/config')

// è§£æå‘½ä»¤è¡Œå‚æ•°
const args = process.argv.slice(2)
const command = args[0]
const params = {}

args.slice(1).forEach((arg) => {
  const [key, value] = arg.split('=')
  params[key.replace('--', '')] = value || true
})

// åˆ›å»º readline æ¥å£
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
})

async function askConfirmation(question) {
  return new Promise((resolve) => {
    rl.question(`${question} (yes/no): `, (answer) => {
      resolve(answer.toLowerCase() === 'yes' || answer.toLowerCase() === 'y')
    })
  })
}

// Claude è´¦æˆ·è§£å¯†å‡½æ•°
function decryptClaudeData(encryptedData) {
  if (!encryptedData || !config.security.encryptionKey) {
    return encryptedData
  }

  try {
    if (encryptedData.includes(':')) {
      const parts = encryptedData.split(':')
      const key = crypto.scryptSync(config.security.encryptionKey, 'salt', 32)
      const iv = Buffer.from(parts[0], 'hex')
      const encrypted = parts[1]

      const decipher = crypto.createDecipheriv('aes-256-cbc', key, iv)
      let decrypted = decipher.update(encrypted, 'hex', 'utf8')
      decrypted += decipher.final('utf8')
      return decrypted
    }
    return encryptedData
  } catch (error) {
    logger.warn(`âš ï¸  Failed to decrypt data: ${error.message}`)
    return encryptedData
  }
}

// Gemini è´¦æˆ·è§£å¯†å‡½æ•°
function decryptGeminiData(encryptedData) {
  if (!encryptedData || !config.security.encryptionKey) {
    return encryptedData
  }

  try {
    if (encryptedData.includes(':')) {
      const parts = encryptedData.split(':')
      const key = crypto.scryptSync(config.security.encryptionKey, 'gemini-account-salt', 32)
      const iv = Buffer.from(parts[0], 'hex')
      const encrypted = parts[1]

      const decipher = crypto.createDecipheriv('aes-256-cbc', key, iv)
      let decrypted = decipher.update(encrypted, 'hex', 'utf8')
      decrypted += decipher.final('utf8')
      return decrypted
    }
    return encryptedData
  } catch (error) {
    logger.warn(`âš ï¸  Failed to decrypt data: ${error.message}`)
    return encryptedData
  }
}

// API Key å“ˆå¸Œå‡½æ•°ï¼ˆä¸apiKeyServiceä¿æŒä¸€è‡´ï¼‰
function hashApiKey(apiKey) {
  if (!apiKey || !config.security.encryptionKey) {
    return apiKey
  }

  return crypto
    .createHash('sha256')
    .update(apiKey + config.security.encryptionKey)
    .digest('hex')
}

// æ£€æŸ¥æ˜¯å¦ä¸ºæ˜æ–‡API Keyï¼ˆé€šè¿‡æ ¼å¼åˆ¤æ–­ï¼Œä¸ä¾èµ–å‰ç¼€ï¼‰
function isPlaintextApiKey(apiKey) {
  if (!apiKey || typeof apiKey !== 'string') {
    return false
  }

  // SHA256å“ˆå¸Œå€¼å›ºå®šä¸º64ä¸ªåå…­è¿›åˆ¶å­—ç¬¦ï¼Œå¦‚æœæ˜¯å“ˆå¸Œå€¼åˆ™è¿”å›false
  if (apiKey.length === 64 && /^[a-f0-9]+$/i.test(apiKey)) {
    return false // å·²ç»æ˜¯å“ˆå¸Œå€¼
  }

  // å…¶ä»–æƒ…å†µéƒ½è®¤ä¸ºæ˜¯æ˜æ–‡API Keyï¼ˆåŒ…æ‹¬sk-ant-ã€cr_ã€è‡ªå®šä¹‰å‰ç¼€ç­‰ï¼‰
  return true
}

// æ•°æ®åŠ å¯†å‡½æ•°ï¼ˆç”¨äºå¯¼å…¥ï¼‰
function encryptClaudeData(data) {
  if (!data || !config.security.encryptionKey) {
    return data
  }

  const key = crypto.scryptSync(config.security.encryptionKey, 'salt', 32)
  const iv = crypto.randomBytes(16)

  const cipher = crypto.createCipheriv('aes-256-cbc', key, iv)
  let encrypted = cipher.update(data, 'utf8', 'hex')
  encrypted += cipher.final('hex')

  return `${iv.toString('hex')}:${encrypted}`
}

function encryptGeminiData(data) {
  if (!data || !config.security.encryptionKey) {
    return data
  }

  const key = crypto.scryptSync(config.security.encryptionKey, 'gemini-account-salt', 32)
  const iv = crypto.randomBytes(16)

  const cipher = crypto.createCipheriv('aes-256-cbc', key, iv)
  let encrypted = cipher.update(data, 'utf8', 'hex')
  encrypted += cipher.final('hex')

  return `${iv.toString('hex')}:${encrypted}`
}

// å¯¼å‡ºä½¿ç”¨ç»Ÿè®¡æ•°æ®
async function exportUsageStats(keyId) {
  try {
    const stats = {
      total: {},
      daily: {},
      monthly: {},
      hourly: {},
      models: {},
      // è´¹ç”¨ç»Ÿè®¡ï¼ˆString ç±»å‹ï¼‰
      costTotal: null,
      costDaily: {},
      costMonthly: {},
      costHourly: {},
      opusTotal: null,
      opusWeekly: {}
    }

    // å¯¼å‡ºæ€»ç»Ÿè®¡ï¼ˆHashï¼‰
    const totalData = await redis.client.hgetall(`usage:${keyId}`)
    if (totalData && Object.keys(totalData).length > 0) {
      stats.total = totalData
    }

    // å¯¼å‡ºè´¹ç”¨æ€»ç»Ÿè®¡ï¼ˆStringï¼‰
    const costTotal = await redis.client.get(`usage:cost:total:${keyId}`)
    if (costTotal) {
      stats.costTotal = costTotal
    }

    // å¯¼å‡º Opus è´¹ç”¨æ€»ç»Ÿè®¡ï¼ˆStringï¼‰
    const opusTotal = await redis.client.get(`usage:opus:total:${keyId}`)
    if (opusTotal) {
      stats.opusTotal = opusTotal
    }

    // å¯¼å‡ºæ¯æ—¥ç»Ÿè®¡ï¼ˆæ‰«æç°æœ‰ keyï¼Œé¿å…æ—¶åŒºé—®é¢˜ï¼‰
    const dailyKeys = await redis.client.keys(`usage:daily:${keyId}:*`)
    for (const key of dailyKeys) {
      const date = key.split(':').pop()
      const data = await redis.client.hgetall(key)
      if (data && Object.keys(data).length > 0) {
        stats.daily[date] = data
      }
    }

    // å¯¼å‡ºæ¯æ—¥è´¹ç”¨ï¼ˆæ‰«æç°æœ‰ keyï¼‰
    const costDailyKeys = await redis.client.keys(`usage:cost:daily:${keyId}:*`)
    for (const key of costDailyKeys) {
      const date = key.split(':').pop()
      const value = await redis.client.get(key)
      if (value) {
        stats.costDaily[date] = value
      }
    }

    // å¯¼å‡ºæ¯æœˆç»Ÿè®¡ï¼ˆæ‰«æç°æœ‰ keyï¼‰
    const monthlyKeys = await redis.client.keys(`usage:monthly:${keyId}:*`)
    for (const key of monthlyKeys) {
      const month = key.split(':').pop()
      const data = await redis.client.hgetall(key)
      if (data && Object.keys(data).length > 0) {
        stats.monthly[month] = data
      }
    }

    // å¯¼å‡ºæ¯æœˆè´¹ç”¨ï¼ˆæ‰«æç°æœ‰ keyï¼‰
    const costMonthlyKeys = await redis.client.keys(`usage:cost:monthly:${keyId}:*`)
    for (const key of costMonthlyKeys) {
      const month = key.split(':').pop()
      const value = await redis.client.get(key)
      if (value) {
        stats.costMonthly[month] = value
      }
    }

    // å¯¼å‡º Opus å‘¨è´¹ç”¨ï¼ˆæ‰«æç°æœ‰ keyï¼‰
    const opusWeeklyKeys = await redis.client.keys(`usage:opus:weekly:${keyId}:*`)
    for (const key of opusWeeklyKeys) {
      const week = key.split(':').pop()
      const value = await redis.client.get(key)
      if (value) {
        stats.opusWeekly[week] = value
      }
    }

    // å¯¼å‡ºå°æ—¶ç»Ÿè®¡ï¼ˆæ‰«æç°æœ‰ keyï¼‰
    // key æ ¼å¼: usage:hourly:{keyId}:{YYYY-MM-DD}:{HH}
    const hourlyKeys = await redis.client.keys(`usage:hourly:${keyId}:*`)
    for (const key of hourlyKeys) {
      const parts = key.split(':')
      const hourKey = `${parts[parts.length - 2]}:${parts[parts.length - 1]}` // YYYY-MM-DD:HH
      const data = await redis.client.hgetall(key)
      if (data && Object.keys(data).length > 0) {
        stats.hourly[hourKey] = data
      }
    }

    // å¯¼å‡ºå°æ—¶è´¹ç”¨ï¼ˆæ‰«æç°æœ‰ keyï¼‰
    // key æ ¼å¼: usage:cost:hourly:{keyId}:{YYYY-MM-DD}:{HH}
    const costHourlyKeys = await redis.client.keys(`usage:cost:hourly:${keyId}:*`)
    for (const key of costHourlyKeys) {
      const parts = key.split(':')
      const hourKey = `${parts[parts.length - 2]}:${parts[parts.length - 1]}` // YYYY-MM-DD:HH
      const value = await redis.client.get(key)
      if (value) {
        stats.costHourly[hourKey] = value
      }
    }

    // å¯¼å‡ºæ¨¡å‹ç»Ÿè®¡ï¼ˆæ¯æ—¥ï¼‰
    const modelDailyKeys = await redis.client.keys(`usage:${keyId}:model:daily:*`)
    for (const key of modelDailyKeys) {
      const match = key.match(/usage:.+:model:daily:(.+):(\d{4}-\d{2}-\d{2})$/)
      if (match) {
        const model = match[1]
        const date = match[2]
        const data = await redis.client.hgetall(key)
        if (data && Object.keys(data).length > 0) {
          if (!stats.models[model]) {
            stats.models[model] = { daily: {}, monthly: {} }
          }
          stats.models[model].daily[date] = data
        }
      }
    }

    // å¯¼å‡ºæ¨¡å‹ç»Ÿè®¡ï¼ˆæ¯æœˆï¼‰
    const modelMonthlyKeys = await redis.client.keys(`usage:${keyId}:model:monthly:*`)
    for (const key of modelMonthlyKeys) {
      const match = key.match(/usage:.+:model:monthly:(.+):(\d{4}-\d{2})$/)
      if (match) {
        const model = match[1]
        const month = match[2]
        const data = await redis.client.hgetall(key)
        if (data && Object.keys(data).length > 0) {
          if (!stats.models[model]) {
            stats.models[model] = { daily: {}, monthly: {} }
          }
          stats.models[model].monthly[month] = data
        }
      }
    }

    return stats
  } catch (error) {
    logger.warn(`âš ï¸  Failed to export usage stats for ${keyId}: ${error.message}`)
    return null
  }
}

// å¯¼å…¥ä½¿ç”¨ç»Ÿè®¡æ•°æ®
async function importUsageStats(keyId, stats) {
  try {
    if (!stats) {
      return
    }

    const pipeline = redis.client.pipeline()
    let importCount = 0

    // å¯¼å…¥æ€»ç»Ÿè®¡ï¼ˆHashï¼‰
    if (stats.total && Object.keys(stats.total).length > 0) {
      for (const [field, value] of Object.entries(stats.total)) {
        pipeline.hset(`usage:${keyId}`, field, value)
      }
      importCount++
    }

    // å¯¼å…¥è´¹ç”¨æ€»ç»Ÿè®¡ï¼ˆStringï¼‰
    if (stats.costTotal) {
      pipeline.set(`usage:cost:total:${keyId}`, stats.costTotal)
      importCount++
    }

    // å¯¼å…¥ Opus è´¹ç”¨æ€»ç»Ÿè®¡ï¼ˆStringï¼‰
    if (stats.opusTotal) {
      pipeline.set(`usage:opus:total:${keyId}`, stats.opusTotal)
      importCount++
    }

    // å¯¼å…¥æ¯æ—¥ç»Ÿè®¡ï¼ˆHashï¼‰
    if (stats.daily) {
      for (const [date, data] of Object.entries(stats.daily)) {
        for (const [field, value] of Object.entries(data)) {
          pipeline.hset(`usage:daily:${keyId}:${date}`, field, value)
        }
        importCount++
      }
    }

    // å¯¼å…¥æ¯æ—¥è´¹ç”¨ï¼ˆStringï¼‰
    if (stats.costDaily) {
      for (const [date, value] of Object.entries(stats.costDaily)) {
        pipeline.set(`usage:cost:daily:${keyId}:${date}`, value)
        importCount++
      }
    }

    // å¯¼å…¥æ¯æœˆç»Ÿè®¡ï¼ˆHashï¼‰
    if (stats.monthly) {
      for (const [month, data] of Object.entries(stats.monthly)) {
        for (const [field, value] of Object.entries(data)) {
          pipeline.hset(`usage:monthly:${keyId}:${month}`, field, value)
        }
        importCount++
      }
    }

    // å¯¼å…¥æ¯æœˆè´¹ç”¨ï¼ˆStringï¼‰
    if (stats.costMonthly) {
      for (const [month, value] of Object.entries(stats.costMonthly)) {
        pipeline.set(`usage:cost:monthly:${keyId}:${month}`, value)
        importCount++
      }
    }

    // å¯¼å…¥ Opus å‘¨è´¹ç”¨ï¼ˆStringï¼Œä¸åŠ  TTL ä¿ç•™å†å²å…¨é‡ï¼‰
    if (stats.opusWeekly) {
      for (const [week, value] of Object.entries(stats.opusWeekly)) {
        pipeline.set(`usage:opus:weekly:${keyId}:${week}`, value)
        importCount++
      }
    }

    // å¯¼å…¥å°æ—¶ç»Ÿè®¡ï¼ˆHashï¼‰
    if (stats.hourly) {
      for (const [hour, data] of Object.entries(stats.hourly)) {
        for (const [field, value] of Object.entries(data)) {
          pipeline.hset(`usage:hourly:${keyId}:${hour}`, field, value)
        }
        importCount++
      }
    }

    // å¯¼å…¥å°æ—¶è´¹ç”¨ï¼ˆStringï¼‰
    if (stats.costHourly) {
      for (const [hour, value] of Object.entries(stats.costHourly)) {
        pipeline.set(`usage:cost:hourly:${keyId}:${hour}`, value)
        importCount++
      }
    }

    // å¯¼å…¥æ¨¡å‹ç»Ÿè®¡ï¼ˆHashï¼‰
    if (stats.models) {
      for (const [model, modelStats] of Object.entries(stats.models)) {
        if (modelStats.daily) {
          for (const [date, data] of Object.entries(modelStats.daily)) {
            for (const [field, value] of Object.entries(data)) {
              pipeline.hset(`usage:${keyId}:model:daily:${model}:${date}`, field, value)
            }
            importCount++
          }
        }

        if (modelStats.monthly) {
          for (const [month, data] of Object.entries(modelStats.monthly)) {
            for (const [field, value] of Object.entries(data)) {
              pipeline.hset(`usage:${keyId}:model:monthly:${model}:${month}`, field, value)
            }
            importCount++
          }
        }
      }
    }

    await pipeline.exec()
    logger.info(`  ğŸ“Š Imported ${importCount} usage stat entries for API Key ${keyId}`)
  } catch (error) {
    logger.warn(`âš ï¸  Failed to import usage stats for ${keyId}: ${error.message}`)
  }
}

// æ•°æ®è„±æ•å‡½æ•°
function sanitizeData(data, type) {
  const sanitized = { ...data }

  switch (type) {
    case 'apikey':
      if (sanitized.apiKey) {
        sanitized.apiKey = `${sanitized.apiKey.substring(0, 10)}...[REDACTED]`
      }
      break

    case 'claude_account':
      if (sanitized.email) {
        sanitized.email = '[REDACTED]'
      }
      if (sanitized.password) {
        sanitized.password = '[REDACTED]'
      }
      if (sanitized.accessToken) {
        sanitized.accessToken = '[REDACTED]'
      }
      if (sanitized.refreshToken) {
        sanitized.refreshToken = '[REDACTED]'
      }
      if (sanitized.claudeAiOauth) {
        sanitized.claudeAiOauth = '[REDACTED]'
      }
      if (sanitized.proxyPassword) {
        sanitized.proxyPassword = '[REDACTED]'
      }
      break

    case 'gemini_account':
      if (sanitized.geminiOauth) {
        sanitized.geminiOauth = '[REDACTED]'
      }
      if (sanitized.accessToken) {
        sanitized.accessToken = '[REDACTED]'
      }
      if (sanitized.refreshToken) {
        sanitized.refreshToken = '[REDACTED]'
      }
      if (sanitized.proxyPassword) {
        sanitized.proxyPassword = '[REDACTED]'
      }
      break

    case 'admin':
      if (sanitized.password) {
        sanitized.password = '[REDACTED]'
      }
      break
  }

  return sanitized
}

// å¯¼å‡ºæ•°æ®
async function exportData() {
  try {
    const outputFile = params.output || `backup-${new Date().toISOString().split('T')[0]}.json`
    const types = params.types ? params.types.split(',') : ['all']
    const shouldSanitize = params.sanitize === true
    const shouldDecrypt = params.decrypt !== false // é»˜è®¤è§£å¯†

    logger.info('ğŸ”„ Starting data export...')
    logger.info(`ğŸ“ Output file: ${outputFile}`)
    logger.info(`ğŸ“‹ Data types: ${types.join(', ')}`)
    logger.info(`ğŸ”’ Sanitize sensitive data: ${shouldSanitize ? 'YES' : 'NO'}`)
    logger.info(`ğŸ”“ Decrypt data: ${shouldDecrypt ? 'YES' : 'NO'}`)

    await redis.connect()
    logger.success('âœ… Connected to Redis')

    const exportDataObj = {
      metadata: {
        version: '2.0',
        exportDate: new Date().toISOString(),
        sanitized: shouldSanitize,
        decrypted: shouldDecrypt,
        types
      },
      data: {}
    }

    // å¯¼å‡º API Keys
    if (types.includes('all') || types.includes('apikeys')) {
      logger.info('ğŸ“¤ Exporting API Keys...')
      const keys = await redis.client.keys('apikey:*')
      const apiKeys = []

      for (const key of keys) {
        if (key === 'apikey:hash_map') {
          continue
        }

        const data = await redis.client.hgetall(key)
        if (data && Object.keys(data).length > 0) {
          // è·å–è¯¥ API Key çš„ ID
          const keyId = data.id

          // å¯¼å‡ºä½¿ç”¨ç»Ÿè®¡æ•°æ®
          if (keyId && (types.includes('all') || types.includes('stats'))) {
            data.usageStats = await exportUsageStats(keyId)
          }

          apiKeys.push(shouldSanitize ? sanitizeData(data, 'apikey') : data)
        }
      }

      exportDataObj.data.apiKeys = apiKeys
      logger.success(`âœ… Exported ${apiKeys.length} API Keys`)
    }

    // å¯¼å‡º Claude è´¦æˆ·
    if (types.includes('all') || types.includes('accounts')) {
      logger.info('ğŸ“¤ Exporting Claude accounts...')
      const keys = await redis.client.keys('claude:account:*')
      logger.info(`Found ${keys.length} Claude account keys in Redis`)
      const accounts = []

      for (const key of keys) {
        const data = await redis.client.hgetall(key)

        if (data && Object.keys(data).length > 0) {
          // è§£å¯†æ•æ„Ÿå­—æ®µ
          if (shouldDecrypt && !shouldSanitize) {
            if (data.email) {
              data.email = decryptClaudeData(data.email)
            }
            if (data.password) {
              data.password = decryptClaudeData(data.password)
            }
            if (data.accessToken) {
              data.accessToken = decryptClaudeData(data.accessToken)
            }
            if (data.refreshToken) {
              data.refreshToken = decryptClaudeData(data.refreshToken)
            }
            if (data.claudeAiOauth) {
              const decrypted = decryptClaudeData(data.claudeAiOauth)
              try {
                data.claudeAiOauth = JSON.parse(decrypted)
              } catch (e) {
                data.claudeAiOauth = decrypted
              }
            }
          }

          accounts.push(shouldSanitize ? sanitizeData(data, 'claude_account') : data)
        }
      }

      exportDataObj.data.claudeAccounts = accounts
      logger.success(`âœ… Exported ${accounts.length} Claude accounts`)

      // å¯¼å‡º Gemini è´¦æˆ·
      logger.info('ğŸ“¤ Exporting Gemini accounts...')
      const geminiKeys = await redis.client.keys('gemini_account:*')
      logger.info(`Found ${geminiKeys.length} Gemini account keys in Redis`)
      const geminiAccounts = []

      for (const key of geminiKeys) {
        const data = await redis.client.hgetall(key)

        if (data && Object.keys(data).length > 0) {
          // è§£å¯†æ•æ„Ÿå­—æ®µ
          if (shouldDecrypt && !shouldSanitize) {
            if (data.geminiOauth) {
              const decrypted = decryptGeminiData(data.geminiOauth)
              try {
                data.geminiOauth = JSON.parse(decrypted)
              } catch (e) {
                data.geminiOauth = decrypted
              }
            }
            if (data.accessToken) {
              data.accessToken = decryptGeminiData(data.accessToken)
            }
            if (data.refreshToken) {
              data.refreshToken = decryptGeminiData(data.refreshToken)
            }
          }

          geminiAccounts.push(shouldSanitize ? sanitizeData(data, 'gemini_account') : data)
        }
      }

      exportDataObj.data.geminiAccounts = geminiAccounts
      logger.success(`âœ… Exported ${geminiAccounts.length} Gemini accounts`)
    }

    // å¯¼å‡ºç®¡ç†å‘˜
    if (types.includes('all') || types.includes('admins')) {
      logger.info('ğŸ“¤ Exporting admins...')
      const keys = await redis.client.keys('admin:*')
      const admins = []

      for (const key of keys) {
        if (key.includes('admin_username:')) {
          continue
        }

        const data = await redis.client.hgetall(key)
        if (data && Object.keys(data).length > 0) {
          admins.push(shouldSanitize ? sanitizeData(data, 'admin') : data)
        }
      }

      exportDataObj.data.admins = admins
      logger.success(`âœ… Exported ${admins.length} admins`)
    }

    // å¯¼å‡ºå…¨å±€æ¨¡å‹ç»Ÿè®¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if (types.includes('all') || types.includes('stats')) {
      logger.info('ğŸ“¤ Exporting global model statistics...')
      const globalStats = {
        daily: {},
        monthly: {},
        hourly: {},
        // æ–°å¢ï¼šç´¢å¼•å’Œå…¨å±€ç»Ÿè®¡
        monthlyMonths: [], // usage:model:monthly:months Set
        globalTotal: null, // usage:global:total Hash
        globalDaily: {}, // usage:global:daily:* Hash
        globalMonthly: {} // usage:global:monthly:* Hash
      }

      // å¯¼å‡ºæœˆä»½ç´¢å¼•
      const monthlyMonths = await redis.client.smembers('usage:model:monthly:months')
      if (monthlyMonths && monthlyMonths.length > 0) {
        globalStats.monthlyMonths = monthlyMonths
        logger.info(`ğŸ“¤ Found ${monthlyMonths.length} months in index`)
      }

      // å¯¼å‡ºå…¨å±€ç»Ÿè®¡
      const globalTotal = await redis.client.hgetall('usage:global:total')
      if (globalTotal && Object.keys(globalTotal).length > 0) {
        globalStats.globalTotal = globalTotal
        logger.info('ğŸ“¤ Found global total stats')
      }

      // å¯¼å‡ºå…¨å±€æ¯æ—¥ç»Ÿè®¡
      const globalDailyKeys = await redis.client.keys('usage:global:daily:*')
      for (const key of globalDailyKeys) {
        const date = key.replace('usage:global:daily:', '')
        const data = await redis.client.hgetall(key)
        if (data && Object.keys(data).length > 0) {
          globalStats.globalDaily[date] = data
        }
      }
      logger.info(`ğŸ“¤ Found ${Object.keys(globalStats.globalDaily).length} global daily stats`)

      // å¯¼å‡ºå…¨å±€æ¯æœˆç»Ÿè®¡
      const globalMonthlyKeys = await redis.client.keys('usage:global:monthly:*')
      for (const key of globalMonthlyKeys) {
        const month = key.replace('usage:global:monthly:', '')
        const data = await redis.client.hgetall(key)
        if (data && Object.keys(data).length > 0) {
          globalStats.globalMonthly[month] = data
        }
      }
      logger.info(`ğŸ“¤ Found ${Object.keys(globalStats.globalMonthly).length} global monthly stats`)

      // å¯¼å‡ºå…¨å±€æ¯æ—¥æ¨¡å‹ç»Ÿè®¡
      const modelDailyPattern = 'usage:model:daily:*'
      const modelDailyKeys = await redis.client.keys(modelDailyPattern)
      for (const key of modelDailyKeys) {
        const match = key.match(/usage:model:daily:(.+):(\d{4}-\d{2}-\d{2})$/)
        if (match) {
          const model = match[1]
          const date = match[2]
          const data = await redis.client.hgetall(key)
          if (data && Object.keys(data).length > 0) {
            if (!globalStats.daily[date]) {
              globalStats.daily[date] = {}
            }
            globalStats.daily[date][model] = data
          }
        }
      }

      // å¯¼å‡ºå…¨å±€æ¯æœˆæ¨¡å‹ç»Ÿè®¡
      const modelMonthlyPattern = 'usage:model:monthly:*'
      const modelMonthlyKeys = await redis.client.keys(modelMonthlyPattern)
      for (const key of modelMonthlyKeys) {
        const match = key.match(/usage:model:monthly:(.+):(\d{4}-\d{2})$/)
        if (match) {
          const model = match[1]
          const month = match[2]
          const data = await redis.client.hgetall(key)
          if (data && Object.keys(data).length > 0) {
            if (!globalStats.monthly[month]) {
              globalStats.monthly[month] = {}
            }
            globalStats.monthly[month][model] = data
          }
        }
      }

      // å¯¼å‡ºå…¨å±€æ¯å°æ—¶æ¨¡å‹ç»Ÿè®¡
      const globalHourlyPattern = 'usage:model:hourly:*'
      const globalHourlyKeys = await redis.client.keys(globalHourlyPattern)
      for (const key of globalHourlyKeys) {
        const match = key.match(/usage:model:hourly:(.+):(\d{4}-\d{2}-\d{2}:\d{2})$/)
        if (match) {
          const model = match[1]
          const hour = match[2]
          const data = await redis.client.hgetall(key)
          if (data && Object.keys(data).length > 0) {
            if (!globalStats.hourly[hour]) {
              globalStats.hourly[hour] = {}
            }
            globalStats.hourly[hour][model] = data
          }
        }
      }

      exportDataObj.data.globalModelStats = globalStats
      logger.success('âœ… Exported global model statistics')
    }

    // å†™å…¥æ–‡ä»¶
    await fs.writeFile(outputFile, JSON.stringify(exportDataObj, null, 2))

    // æ˜¾ç¤ºå¯¼å‡ºæ‘˜è¦
    console.log(`\n${'='.repeat(60)}`)
    console.log('âœ… Export Complete!')
    console.log('='.repeat(60))
    console.log(`Output file: ${outputFile}`)
    console.log(`File size: ${(await fs.stat(outputFile)).size} bytes`)

    if (exportDataObj.data.apiKeys) {
      console.log(`API Keys: ${exportDataObj.data.apiKeys.length}`)
    }
    if (exportDataObj.data.claudeAccounts) {
      console.log(`Claude Accounts: ${exportDataObj.data.claudeAccounts.length}`)
    }
    if (exportDataObj.data.geminiAccounts) {
      console.log(`Gemini Accounts: ${exportDataObj.data.geminiAccounts.length}`)
    }
    if (exportDataObj.data.admins) {
      console.log(`Admins: ${exportDataObj.data.admins.length}`)
    }
    console.log('='.repeat(60))

    if (shouldSanitize) {
      logger.warn('âš ï¸  Sensitive data has been sanitized in this export.')
    }
    if (shouldDecrypt) {
      logger.info('ğŸ”“ Encrypted data has been decrypted for portability.')
    }
  } catch (error) {
    logger.error('ğŸ’¥ Export failed:', error)
    process.exit(1)
  } finally {
    await redis.disconnect()
    rl.close()
  }
}

// æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
function showHelp() {
  console.log(`
Enhanced Data Transfer Tool for Claude Relay Service

This tool handles encrypted data export/import between environments.

Usage:
  node scripts/data-transfer-enhanced.js <command> [options]

Commands:
  export    Export data from Redis to a JSON file
  import    Import data from a JSON file to Redis

Export Options:
  --output=FILE        Output filename (default: backup-YYYY-MM-DD.json)
  --types=TYPE,...     Data types: apikeys,accounts,admins,stats,all (default: all)
                       stats: Include usage statistics with API keys
  --sanitize           Remove sensitive data from export
  --decrypt=false      Keep data encrypted (default: true - decrypt for portability)

Import Options:
  --input=FILE         Input filename (required)
  --force              Overwrite existing data without asking
  --skip-conflicts     Skip conflicting data without asking

Important Notes:
  - The tool automatically handles encryption/decryption during import
  - If importing decrypted data, it will be re-encrypted automatically
  - If importing encrypted data, it will be stored as-is
  - Sanitized exports cannot be properly imported (missing sensitive data)
  - Automatic handling of plaintext API Keys
    * Uses your configured API_KEY_PREFIX from config (sk-, cr_, etc.)
    * Automatically detects plaintext vs hashed API Keys by format
    * Plaintext API Keys are automatically hashed during import
    * Hash mappings are created correctly for plaintext keys
    * Supports custom prefixes and legacy format detection
    * No manual conversion needed - just import your backup file

Examples:
  # Export all data with decryption (for migration)
  node scripts/data-transfer-enhanced.js export

  # Export without decrypting (for backup)
  node scripts/data-transfer-enhanced.js export --decrypt=false

  # Import data (auto-handles encryption and plaintext API keys)
  node scripts/data-transfer-enhanced.js import --input=backup.json

  # Import with force overwrite
  node scripts/data-transfer-enhanced.js import --input=backup.json --force
`)
}

// å¯¼å…¥æ•°æ®
async function importData() {
  try {
    const inputFile = params.input
    if (!inputFile) {
      logger.error('âŒ Please specify input file with --input=filename.json')
      process.exit(1)
    }

    const forceOverwrite = params.force === true
    const skipConflicts = params['skip-conflicts'] === true

    logger.info('ğŸ”„ Starting data import...')
    logger.info(`ğŸ“ Input file: ${inputFile}`)
    logger.info(
      `âš¡ Mode: ${forceOverwrite ? 'FORCE OVERWRITE' : skipConflicts ? 'SKIP CONFLICTS' : 'ASK ON CONFLICT'}`
    )

    // è¯»å–æ–‡ä»¶
    const fileContent = await fs.readFile(inputFile, 'utf8')
    const importDataObj = JSON.parse(fileContent)

    // éªŒè¯æ–‡ä»¶æ ¼å¼
    if (!importDataObj.metadata || !importDataObj.data) {
      logger.error('âŒ Invalid backup file format')
      process.exit(1)
    }

    logger.info(`ğŸ“… Backup date: ${importDataObj.metadata.exportDate}`)
    logger.info(`ğŸ”’ Sanitized: ${importDataObj.metadata.sanitized ? 'YES' : 'NO'}`)
    logger.info(`ğŸ”“ Decrypted: ${importDataObj.metadata.decrypted ? 'YES' : 'NO'}`)

    if (importDataObj.metadata.sanitized) {
      logger.warn('âš ï¸  This backup contains sanitized data. Sensitive fields will be missing!')
      const proceed = await askConfirmation('Continue with sanitized data?')
      if (!proceed) {
        logger.info('âŒ Import cancelled')
        return
      }
    }

    // æ˜¾ç¤ºå¯¼å…¥æ‘˜è¦
    console.log(`\n${'='.repeat(60)}`)
    console.log('ğŸ“‹ Import Summary:')
    console.log('='.repeat(60))
    if (importDataObj.data.apiKeys) {
      console.log(`API Keys to import: ${importDataObj.data.apiKeys.length}`)
    }
    if (importDataObj.data.claudeAccounts) {
      console.log(`Claude Accounts to import: ${importDataObj.data.claudeAccounts.length}`)
    }
    if (importDataObj.data.geminiAccounts) {
      console.log(`Gemini Accounts to import: ${importDataObj.data.geminiAccounts.length}`)
    }
    if (importDataObj.data.admins) {
      console.log(`Admins to import: ${importDataObj.data.admins.length}`)
    }
    console.log(`${'='.repeat(60)}\n`)

    // ç¡®è®¤å¯¼å…¥
    const confirmed = await askConfirmation('âš ï¸  Proceed with import?')
    if (!confirmed) {
      logger.info('âŒ Import cancelled')
      return
    }

    // è¿æ¥ Redis
    await redis.connect()
    logger.success('âœ… Connected to Redis')

    const stats = {
      imported: 0,
      skipped: 0,
      errors: 0
    }

    // å¯¼å…¥ API Keys
    if (importDataObj.data.apiKeys) {
      logger.info('\nğŸ“¥ Importing API Keys...')
      for (const apiKey of importDataObj.data.apiKeys) {
        try {
          const exists = await redis.client.exists(`apikey:${apiKey.id}`)

          if (exists && !forceOverwrite) {
            if (skipConflicts) {
              logger.warn(`â­ï¸  Skipped existing API Key: ${apiKey.name} (${apiKey.id})`)
              stats.skipped++
              continue
            } else {
              const overwrite = await askConfirmation(
                `API Key "${apiKey.name}" (${apiKey.id}) exists. Overwrite?`
              )
              if (!overwrite) {
                stats.skipped++
                continue
              }
            }
          }

          // ä¿å­˜ä½¿ç”¨ç»Ÿè®¡æ•°æ®ä»¥ä¾¿å•ç‹¬å¯¼å…¥
          const { usageStats } = apiKey

          // ä»apiKeyå¯¹è±¡ä¸­åˆ é™¤usageStatså­—æ®µï¼Œé¿å…å­˜å‚¨åˆ°ä¸»é”®ä¸­
          const apiKeyData = { ...apiKey }
          delete apiKeyData.usageStats

          // æ£€æŸ¥å¹¶å¤„ç†API Keyå“ˆå¸Œ
          let plainTextApiKey = null
          let hashedApiKey = null

          if (apiKeyData.apiKey && isPlaintextApiKey(apiKeyData.apiKey)) {
            // å¦‚æœæ˜¯æ˜æ–‡API Keyï¼Œä¿å­˜æ˜æ–‡å¹¶è®¡ç®—å“ˆå¸Œ
            plainTextApiKey = apiKeyData.apiKey
            hashedApiKey = hashApiKey(plainTextApiKey)
            logger.info(`ğŸ” Detected plaintext API Key for: ${apiKey.name} (${apiKey.id})`)
          } else if (apiKeyData.apiKey) {
            // å¦‚æœå·²ç»æ˜¯å“ˆå¸Œå€¼ï¼Œç›´æ¥ä½¿ç”¨
            hashedApiKey = apiKeyData.apiKey
            logger.info(`ğŸ” Using existing hashed API Key for: ${apiKey.name} (${apiKey.id})`)
          }

          // API Keyå­—æ®µå§‹ç»ˆå­˜å‚¨å“ˆå¸Œå€¼
          if (hashedApiKey) {
            apiKeyData.apiKey = hashedApiKey
          }

          // ä½¿ç”¨ hset å­˜å‚¨åˆ°å“ˆå¸Œè¡¨
          const pipeline = redis.client.pipeline()
          for (const [field, value] of Object.entries(apiKeyData)) {
            pipeline.hset(`apikey:${apiKey.id}`, field, value)
          }
          await pipeline.exec()

          // æ›´æ–°å“ˆå¸Œæ˜ å°„ï¼šhash_mapçš„keyå¿…é¡»æ˜¯å“ˆå¸Œå€¼
          if (!importDataObj.metadata.sanitized && hashedApiKey) {
            await redis.client.hset('apikey:hash_map', hashedApiKey, apiKey.id)
            logger.info(
              `ğŸ“ Updated hash mapping: ${hashedApiKey.substring(0, 8)}... -> ${apiKey.id}`
            )
          }

          // å¯¼å…¥ä½¿ç”¨ç»Ÿè®¡æ•°æ®
          if (usageStats) {
            await importUsageStats(apiKey.id, usageStats)
          }

          logger.success(`âœ… Imported API Key: ${apiKey.name} (${apiKey.id})`)
          stats.imported++
        } catch (error) {
          logger.error(`âŒ Failed to import API Key ${apiKey.id}:`, error.message)
          stats.errors++
        }
      }
    }

    // å¯¼å…¥ Claude è´¦æˆ·
    if (importDataObj.data.claudeAccounts) {
      logger.info('\nğŸ“¥ Importing Claude accounts...')
      for (const account of importDataObj.data.claudeAccounts) {
        try {
          const exists = await redis.client.exists(`claude:account:${account.id}`)

          if (exists && !forceOverwrite) {
            if (skipConflicts) {
              logger.warn(`â­ï¸  Skipped existing Claude account: ${account.name} (${account.id})`)
              stats.skipped++
              continue
            } else {
              const overwrite = await askConfirmation(
                `Claude account "${account.name}" (${account.id}) exists. Overwrite?`
              )
              if (!overwrite) {
                stats.skipped++
                continue
              }
            }
          }

          // å¤åˆ¶è´¦æˆ·æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
          const accountData = { ...account }

          // å¦‚æœæ•°æ®å·²è§£å¯†ä¸”ä¸æ˜¯è„±æ•æ•°æ®ï¼Œéœ€è¦é‡æ–°åŠ å¯†
          if (importDataObj.metadata.decrypted && !importDataObj.metadata.sanitized) {
            logger.info(`ğŸ” Re-encrypting sensitive data for Claude account: ${account.name}`)

            if (accountData.email) {
              accountData.email = encryptClaudeData(accountData.email)
            }
            if (accountData.password) {
              accountData.password = encryptClaudeData(accountData.password)
            }
            if (accountData.accessToken) {
              accountData.accessToken = encryptClaudeData(accountData.accessToken)
            }
            if (accountData.refreshToken) {
              accountData.refreshToken = encryptClaudeData(accountData.refreshToken)
            }
            if (accountData.claudeAiOauth) {
              // å¦‚æœæ˜¯å¯¹è±¡ï¼Œå…ˆåºåˆ—åŒ–å†åŠ å¯†
              const oauthStr =
                typeof accountData.claudeAiOauth === 'object'
                  ? JSON.stringify(accountData.claudeAiOauth)
                  : accountData.claudeAiOauth
              accountData.claudeAiOauth = encryptClaudeData(oauthStr)
            }
          }

          // ä½¿ç”¨ hset å­˜å‚¨åˆ°å“ˆå¸Œè¡¨
          const pipeline = redis.client.pipeline()
          for (const [field, value] of Object.entries(accountData)) {
            if (field === 'claudeAiOauth' && typeof value === 'object') {
              // ç¡®ä¿å¯¹è±¡è¢«åºåˆ—åŒ–
              pipeline.hset(`claude:account:${account.id}`, field, JSON.stringify(value))
            } else {
              pipeline.hset(`claude:account:${account.id}`, field, value)
            }
          }
          await pipeline.exec()

          logger.success(`âœ… Imported Claude account: ${account.name} (${account.id})`)
          stats.imported++
        } catch (error) {
          logger.error(`âŒ Failed to import Claude account ${account.id}:`, error.message)
          stats.errors++
        }
      }
    }

    // å¯¼å…¥ Gemini è´¦æˆ·
    if (importDataObj.data.geminiAccounts) {
      logger.info('\nğŸ“¥ Importing Gemini accounts...')
      for (const account of importDataObj.data.geminiAccounts) {
        try {
          const exists = await redis.client.exists(`gemini_account:${account.id}`)

          if (exists && !forceOverwrite) {
            if (skipConflicts) {
              logger.warn(`â­ï¸  Skipped existing Gemini account: ${account.name} (${account.id})`)
              stats.skipped++
              continue
            } else {
              const overwrite = await askConfirmation(
                `Gemini account "${account.name}" (${account.id}) exists. Overwrite?`
              )
              if (!overwrite) {
                stats.skipped++
                continue
              }
            }
          }

          // å¤åˆ¶è´¦æˆ·æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
          const accountData = { ...account }

          // å¦‚æœæ•°æ®å·²è§£å¯†ä¸”ä¸æ˜¯è„±æ•æ•°æ®ï¼Œéœ€è¦é‡æ–°åŠ å¯†
          if (importDataObj.metadata.decrypted && !importDataObj.metadata.sanitized) {
            logger.info(`ğŸ” Re-encrypting sensitive data for Gemini account: ${account.name}`)

            if (accountData.geminiOauth) {
              const oauthStr =
                typeof accountData.geminiOauth === 'object'
                  ? JSON.stringify(accountData.geminiOauth)
                  : accountData.geminiOauth
              accountData.geminiOauth = encryptGeminiData(oauthStr)
            }
            if (accountData.accessToken) {
              accountData.accessToken = encryptGeminiData(accountData.accessToken)
            }
            if (accountData.refreshToken) {
              accountData.refreshToken = encryptGeminiData(accountData.refreshToken)
            }
          }

          // ä½¿ç”¨ hset å­˜å‚¨åˆ°å“ˆå¸Œè¡¨
          const pipeline = redis.client.pipeline()
          for (const [field, value] of Object.entries(accountData)) {
            pipeline.hset(`gemini_account:${account.id}`, field, value)
          }
          await pipeline.exec()

          logger.success(`âœ… Imported Gemini account: ${account.name} (${account.id})`)
          stats.imported++
        } catch (error) {
          logger.error(`âŒ Failed to import Gemini account ${account.id}:`, error.message)
          stats.errors++
        }
      }
    }

    // å¯¼å…¥ç®¡ç†å‘˜è´¦æˆ·
    if (importDataObj.data.admins) {
      logger.info('\nğŸ“¥ Importing admins...')
      for (const admin of importDataObj.data.admins) {
        try {
          const exists = await redis.client.exists(`admin:${admin.id}`)

          if (exists && !forceOverwrite) {
            if (skipConflicts) {
              logger.warn(`â­ï¸  Skipped existing admin: ${admin.username} (${admin.id})`)
              stats.skipped++
              continue
            } else {
              const overwrite = await askConfirmation(
                `Admin "${admin.username}" (${admin.id}) exists. Overwrite?`
              )
              if (!overwrite) {
                stats.skipped++
                continue
              }
            }
          }

          // ä½¿ç”¨ hset å­˜å‚¨åˆ°å“ˆå¸Œè¡¨
          const pipeline = redis.client.pipeline()
          for (const [field, value] of Object.entries(admin)) {
            pipeline.hset(`admin:${admin.id}`, field, value)
          }
          await pipeline.exec()

          // æ›´æ–°ç”¨æˆ·åæ˜ å°„
          await redis.client.set(`admin_username:${admin.username}`, admin.id)

          logger.success(`âœ… Imported admin: ${admin.username} (${admin.id})`)
          stats.imported++
        } catch (error) {
          logger.error(`âŒ Failed to import admin ${admin.id}:`, error.message)
          stats.errors++
        }
      }
    }

    // å¯¼å…¥å…¨å±€æ¨¡å‹ç»Ÿè®¡
    if (importDataObj.data.globalModelStats) {
      logger.info('\nğŸ“¥ Importing global model statistics...')
      try {
        const globalStats = importDataObj.data.globalModelStats
        const pipeline = redis.client.pipeline()
        let globalStatCount = 0

        // å¯¼å…¥æœˆä»½ç´¢å¼•
        if (globalStats.monthlyMonths && globalStats.monthlyMonths.length > 0) {
          for (const month of globalStats.monthlyMonths) {
            pipeline.sadd('usage:model:monthly:months', month)
          }
          logger.info(`ğŸ“¥ Importing ${globalStats.monthlyMonths.length} months to index`)
        }

        // å¯¼å…¥å…¨å±€ç»Ÿè®¡
        if (globalStats.globalTotal) {
          for (const [field, value] of Object.entries(globalStats.globalTotal)) {
            pipeline.hset('usage:global:total', field, value)
          }
          logger.info('ğŸ“¥ Importing global total stats')
        }

        // å¯¼å…¥å…¨å±€æ¯æ—¥ç»Ÿè®¡
        if (globalStats.globalDaily) {
          for (const [date, data] of Object.entries(globalStats.globalDaily)) {
            for (const [field, value] of Object.entries(data)) {
              pipeline.hset(`usage:global:daily:${date}`, field, value)
            }
          }
          logger.info(
            `ğŸ“¥ Importing ${Object.keys(globalStats.globalDaily).length} global daily stats`
          )
        }

        // å¯¼å…¥å…¨å±€æ¯æœˆç»Ÿè®¡
        if (globalStats.globalMonthly) {
          for (const [month, data] of Object.entries(globalStats.globalMonthly)) {
            for (const [field, value] of Object.entries(data)) {
              pipeline.hset(`usage:global:monthly:${month}`, field, value)
            }
          }
          logger.info(
            `ğŸ“¥ Importing ${Object.keys(globalStats.globalMonthly).length} global monthly stats`
          )
        }

        // å¯¼å…¥æ¯æ—¥ç»Ÿè®¡
        if (globalStats.daily) {
          for (const [date, models] of Object.entries(globalStats.daily)) {
            for (const [model, data] of Object.entries(models)) {
              for (const [field, value] of Object.entries(data)) {
                pipeline.hset(`usage:model:daily:${model}:${date}`, field, value)
              }
              globalStatCount++
            }
          }
        }

        // å¯¼å…¥æ¯æœˆç»Ÿè®¡
        if (globalStats.monthly) {
          for (const [month, models] of Object.entries(globalStats.monthly)) {
            for (const [model, data] of Object.entries(models)) {
              for (const [field, value] of Object.entries(data)) {
                pipeline.hset(`usage:model:monthly:${model}:${month}`, field, value)
              }
              globalStatCount++
            }
            // åŒæ—¶æ›´æ–°æœˆä»½ç´¢å¼•ï¼ˆå…¼å®¹æ—§æ ¼å¼å¯¼å‡ºæ–‡ä»¶ï¼‰
            pipeline.sadd('usage:model:monthly:months', month)
          }
        }

        // å¯¼å…¥æ¯å°æ—¶ç»Ÿè®¡
        if (globalStats.hourly) {
          for (const [hour, models] of Object.entries(globalStats.hourly)) {
            for (const [model, data] of Object.entries(models)) {
              for (const [field, value] of Object.entries(data)) {
                pipeline.hset(`usage:model:hourly:${model}:${hour}`, field, value)
              }
              globalStatCount++
            }
          }
        }

        await pipeline.exec()
        logger.success(`âœ… Imported ${globalStatCount} global model stat entries`)
        stats.imported += globalStatCount
      } catch (error) {
        logger.error('âŒ Failed to import global model stats:', error.message)
        stats.errors++
      }
    }

    // æ˜¾ç¤ºå¯¼å…¥ç»“æœ
    console.log(`\n${'='.repeat(60)}`)
    console.log('âœ… Import Complete!')
    console.log('='.repeat(60))
    console.log(`Successfully imported: ${stats.imported}`)
    console.log(`Skipped: ${stats.skipped}`)
    console.log(`Errors: ${stats.errors}`)
    console.log('='.repeat(60))
  } catch (error) {
    logger.error('ğŸ’¥ Import failed:', error)
    process.exit(1)
  } finally {
    await redis.disconnect()
    rl.close()
  }
}

// ä¸»å‡½æ•°
async function main() {
  if (!command || command === '--help' || command === 'help') {
    showHelp()
    process.exit(0)
  }

  switch (command) {
    case 'export':
      await exportData()
      break

    case 'import':
      await importData()
      break

    default:
      logger.error(`âŒ Unknown command: ${command}`)
      showHelp()
      process.exit(1)
  }
}

// è¿è¡Œ
main().catch((error) => {
  logger.error('ğŸ’¥ Unexpected error:', error)
  process.exit(1)
})
